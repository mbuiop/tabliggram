"""
Ù…ÙˆØªÙˆØ± Ú¯Ø±Ø§Ù Ø¯Ø§Ù†Ø´ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ù‚Ø§Ù„Ø§Øª Ùˆ Ø§Ø³Ù†Ø§Ø¯
"""
import numpy as np
import torch
from typing import Dict, List, Tuple, Optional, Any, Set
from dataclasses import dataclass, field
from collections import defaultdict, deque
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import faiss
import pickle
import json
import hashlib
import asyncio
from concurrent.futures import ThreadPoolExecutor
import redis
import sqlite3
from datetime import datetime, timedelta
import spacy
from transformers import AutoTokenizer, AutoModel
import hnswlib
import mmap
import os
from pathlib import Path
import threading
import queue
import heapq
from enum import Enum

class NodeType(Enum):
    """Ù†ÙˆØ¹ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø±Ø§Ù Ø¯Ø§Ù†Ø´"""
    DOCUMENT = "document"
    CONCEPT = "concept"
    ENTITY = "entity"
    TOPIC = "topic"
    KEYWORD = "keyword"
    RELATIONSHIP = "relationship"
    QUERY = "query"
    RESPONSE = "response"

class EdgeType(Enum):
    """Ù†ÙˆØ¹ ÛŒØ§Ù„â€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø±Ø§Ù Ø¯Ø§Ù†Ø´"""
    CONTAINS = "contains"
    RELATED_TO = "related_to"
    DERIVED_FROM = "derived_from"
    SIMILAR_TO = "similar_to"
    CAUSES = "causes"
    DEPENDS_ON = "depends_on"
    REFERENCES = "references"
    INSTANCE_OF = "instance_of"

@dataclass
class KnowledgeNode:
    """Ú¯Ø±Ù‡ Ø¯Ø§Ù†Ø´"""
    id: str
    type: NodeType
    content: Any
    embedding: Optional[np.ndarray] = None
    metadata: Dict = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)
    access_count: int = 0
    importance: float = 1.0
    vector_id: Optional[int] = None

@dataclass
class KnowledgeEdge:
    """ÛŒØ§Ù„ Ø¯Ø§Ù†Ø´"""
    source: str
    target: str
    type: EdgeType
    weight: float = 1.0
    metadata: Dict = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)

class AdvancedKnowledgeGraph:
    """Ú¯Ø±Ø§Ù Ø¯Ø§Ù†Ø´ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø±Ø¯Ø§Ø±ÛŒ"""
    
    def __init__(self, dimension: int = 4096):
        self.dimension = dimension
        self.graph = nx.MultiDiGraph()
        self.nodes: Dict[str, KnowledgeNode] = {}
        self.edges: List[KnowledgeEdge] = []
        
        # Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¨Ø§ FAISS
        self.index = faiss.IndexFlatIP(dimension)
        self.index_to_id: Dict[int, str] = {}
        
        # Ø§ÛŒÙ†Ø¯Ú©Ø³ HNSW Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ±
        self.hnsw_index = hnswlib.Index(space='ip', dim=dimension)
        self.hnsw_index.init_index(max_elements=1000000, ef_construction=200, M=48)
        
        # Ú©Ø´ Ùˆ Ø­Ø§ÙØ¸Ù‡
        self.cache = redis.Redis(host='localhost', port=6379, decode_responses=True, db=1)
        self.local_cache = {}
        
        # Ø¯ÛŒØªØ§Ø¨ÛŒØ³ SQLite
        self.db_path = "knowledge_graph.db"
        self._init_database()
        
        # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ NLP
        self.nlp = spacy.load("en_core_web_trf")
        self.tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-base")
        self.encoder = AutoModel.from_pretrained("microsoft/deberta-v3-base")
        
        # ØµÙâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
        self.processing_queue = queue.Queue()
        self.executor = ThreadPoolExecutor(max_workers=8)
        
        # Ø¢Ù…Ø§Ø± Ùˆ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
        self.stats = {
            'total_nodes': 0,
            'total_edges': 0,
            'queries_performed': 0,
            'avg_query_time': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        
        # Ù‚ÙÙ„â€ŒÙ‡Ø§
        self.write_lock = threading.RLock()
        self.index_lock = threading.Lock()
        
        # Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡
        self.running = True
        self.background_processor = threading.Thread(target=self._process_queue, daemon=True)
        self.background_processor.start()
    
    def _init_database(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ SQLite"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Ø¬Ø¯ÙˆÙ„ Ú¯Ø±Ù‡â€ŒÙ‡Ø§
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS nodes (
                id TEXT PRIMARY KEY,
                type TEXT,
                content BLOB,
                embedding BLOB,
                metadata TEXT,
                timestamp TIMESTAMP,
                access_count INTEGER,
                importance REAL,
                vector_id INTEGER
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ ÛŒØ§Ù„â€ŒÙ‡Ø§
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS edges (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source TEXT,
                target TEXT,
                type TEXT,
                weight REAL,
                metadata TEXT,
                timestamp TIMESTAMP,
                FOREIGN KEY (source) REFERENCES nodes(id),
                FOREIGN KEY (target) REFERENCES nodes(id)
            )
        ''')
        
        # Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_nodes_type ON nodes(type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_nodes_vector ON nodes(vector_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_edges_source ON edges(source)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_edges_target ON edges(target)')
        
        conn.commit()
        conn.close()
    
    async def add_document(self, content: str, metadata: Dict = None) -> str:
        """Ø§ÙØ²ÙˆØ¯Ù† Ø³Ù†Ø¯ Ø¨Ù‡ Ú¯Ø±Ø§Ù Ø¯Ø§Ù†Ø´"""
        doc_id = hashlib.sha256(content.encode()).hexdigest()
        
        # Ø§ÛŒØ¬Ø§Ø¯ embedding
        embedding = await self._create_embedding(content)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø±Ù‡ Ø§ØµÙ„ÛŒ Ø³Ù†Ø¯
        doc_node = KnowledgeNode(
            id=doc_id,
            type=NodeType.DOCUMENT,
            content=content[:1000],  # Ø®Ù„Ø§ØµÙ‡
            embedding=embedding,
            metadata=metadata or {},
            importance=1.0
        )
        
        with self.write_lock:
            # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ù‡ FAISS
            vector_id = len(self.index_to_id)
            self.index.add(np.array([embedding]))
            self.hnsw_index.add_items(np.array([embedding]), np.array([vector_id]))
            doc_node.vector_id = vector_id
            self.index_to_id[vector_id] = doc_id
            
            # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ù‡ Ú¯Ø±Ø§Ù
            self.nodes[doc_id] = doc_node
            self.graph.add_node(doc_id, **doc_node.__dict__)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙØ§Ù‡ÛŒÙ… Ùˆ entities
            concepts = await self._extract_concepts(content)
            entities = await self._extract_entities(content)
            
            # Ø§ÙØ²ÙˆØ¯Ù† Ù…ÙØ§Ù‡ÛŒÙ… Ø¨Ù‡ Ú¯Ø±Ø§Ù
            for concept in concepts:
                concept_id = await self._add_concept(concept)
                self.add_edge(doc_id, concept_id, EdgeType.CONTAINS)
            
            # Ø§ÙØ²ÙˆØ¯Ù† entities Ø¨Ù‡ Ú¯Ø±Ø§Ù
            for entity in entities:
                entity_id = await self._add_entity(entity)
                self.add_edge(doc_id, entity_id, EdgeType.CONTAINS)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        self._save_to_database(doc_node)
        
        self.stats['total_nodes'] += 1
        return doc_id
    
    async def _create_embedding(self, text: str) -> np.ndarray:
        """Ø§ÛŒØ¬Ø§Ø¯ embedding Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†"""
        # ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù†
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        
        # Ø¯Ø±ÛŒØ§ÙØª embedding
        with torch.no_grad():
            outputs = self.encoder(**inputs)
            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        embedding = embedding / np.linalg.norm(embedding)
        
        return embedding
    
    async def _extract_concepts(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙØ§Ù‡ÛŒÙ… Ø§ØµÙ„ÛŒ Ø§Ø² Ù…ØªÙ†"""
        doc = self.nlp(text)
        
        concepts = []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ noun chunks
        for chunk in doc.noun_chunks:
            if len(chunk.text.split()) <= 3:  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø·ÙˆÙ„
                concepts.append(chunk.text.lower())
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ named entities
        for ent in doc.ents:
            concepts.append(ent.text.lower())
        
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        concepts = list(set(concepts))
        
        return concepts[:50]  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯
    
    async def _extract_entities(self, text: str) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…â€ŒØ¯Ø§Ø±"""
        doc = self.nlp(text)
        
        entities = []
        for ent in doc.ents:
            entities.append({
                'text': ent.text,
                'label': ent.label_,
                'start': ent.start_char,
                'end': ent.end_char
            })
        
        return entities
    
    async def _add_concept(self, concept: str) -> str:
        """Ø§ÙØ²ÙˆØ¯Ù† Ù…ÙÙ‡ÙˆÙ… Ø¨Ù‡ Ú¯Ø±Ø§Ù"""
        concept_id = f"concept_{hashlib.md5(concept.encode()).hexdigest()}"
        
        if concept_id not in self.nodes:
            embedding = await self._create_embedding(concept)
            
            concept_node = KnowledgeNode(
                id=concept_id,
                type=NodeType.CONCEPT,
                content=concept,
                embedding=embedding,
                importance=0.8
            )
            
            with self.write_lock:
                vector_id = len(self.index_to_id)
                self.index.add(np.array([embedding]))
                self.hnsw_index.add_items(np.array([embedding]), np.array([vector_id]))
                concept_node.vector_id = vector_id
                self.index_to_id[vector_id] = concept_id
                
                self.nodes[concept_id] = concept_node
                self.graph.add_node(concept_id, **concept_node.__dict__)
            
            self._save_to_database(concept_node)
            self.stats['total_nodes'] += 1
        
        return concept_id
    
    async def _add_entity(self, entity: Dict) -> str:
        """Ø§ÙØ²ÙˆØ¯Ù† Ù…ÙˆØ¬ÙˆØ¯ÛŒØª Ø¨Ù‡ Ú¯Ø±Ø§Ù"""
        entity_text = entity['text']
        entity_id = f"entity_{hashlib.md5(f'{entity_text}_{entity["label"]}'.encode()).hexdigest()}"
        
        if entity_id not in self.nodes:
            embedding = await self._create_embedding(entity_text)
            
            entity_node = KnowledgeNode(
                id=entity_id,
                type=NodeType.ENTITY,
                content=entity,
                embedding=embedding,
                metadata={'label': entity['label']},
                importance=0.9
            )
            
            with self.write_lock:
                vector_id = len(self.index_to_id)
                self.index.add(np.array([embedding]))
                self.hnsw_index.add_items(np.array([embedding]), np.array([vector_id]))
                entity_node.vector_id = vector_id
                self.index_to_id[vector_id] = entity_id
                
                self.nodes[entity_id] = entity_node
                self.graph.add_node(entity_id, **entity_node.__dict__)
            
            self._save_to_database(entity_node)
            self.stats['total_nodes'] += 1
        
        return entity_id
    
    def add_edge(self, source: str, target: str, edge_type: EdgeType, weight: float = 1.0, metadata: Dict = None):
        """Ø§ÙØ²ÙˆØ¯Ù† ÛŒØ§Ù„ Ø¨ÛŒÙ† Ø¯Ùˆ Ú¯Ø±Ù‡"""
        edge = KnowledgeEdge(
            source=source,
            target=target,
            type=edge_type,
            weight=weight,
            metadata=metadata or {}
        )
        
        with self.write_lock:
            self.edges.append(edge)
            self.graph.add_edge(source, target, **edge.__dict__)
            
            # Ø§ÙØ²Ø§ÛŒØ´ Ø§Ù‡Ù…ÛŒØª Ú¯Ø±Ù‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª
            if source in self.nodes:
                self.nodes[source].importance += 0.01
            if target in self.nodes:
                self.nodes[target].importance += 0.01
        
        self.stats['total_edges'] += 1
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        self._save_edge_to_database(edge)
    
    async def search(self, query: str, k: int = 10, node_type: Optional[NodeType] = None) -> List[Dict]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¯Ø± Ú¯Ø±Ø§Ù Ø¯Ø§Ù†Ø´"""
        start_time = datetime.now()
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø´
        cache_key = f"search:{query}:{k}:{node_type}"
        cached = self.cache.get(cache_key)
        if cached:
            self.stats['cache_hits'] += 1
            return json.loads(cached)
        
        self.stats['cache_misses'] += 1
        
        # Ø§ÛŒØ¬Ø§Ø¯ embedding Ø¨Ø±Ø§ÛŒ query
        query_embedding = await self._create_embedding(query)
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± FAISS
        distances, indices = self.index.search(np.array([query_embedding]), k * 2)
        
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx in self.index_to_id:
                node_id = self.index_to_id[idx]
                node = self.nodes.get(node_id)
                
                if node and (node_type is None or node.type == node_type):
                    # Ø¯Ø±ÛŒØ§ÙØª context Ø§Ø² Ú¯Ø±Ø§Ù
                    context = self._get_node_context(node_id, depth=2)
                    
                    results.append({
                        'node': {
                            'id': node.id,
                            'type': node.type.value,
                            'content': node.content,
                            'importance': node.importance,
                            'metadata': node.metadata
                        },
                        'similarity': float(dist),
                        'context': context,
                        'connections': len(list(self.graph.neighbors(node_id)))
                    })
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ø´Ø¨Ø§Ù‡Øª Ùˆ Ø§Ù‡Ù…ÛŒØª
        results.sort(
            key=lambda x: x['similarity'] * 0.7 + x['node']['importance'] * 0.3,
            reverse=True
        )
        
        results = results[:k]
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
        self.cache.setex(cache_key, timedelta(hours=1), json.dumps(results))
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
        self.stats['queries_performed'] += 1
        query_time = (datetime.now() - start_time).total_seconds()
        self.stats['avg_query_time'] = 0.9 * self.stats['avg_query_time'] + 0.1 * query_time
        
        return results
    
    def _get_node_context(self, node_id: str, depth: int = 2) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª context ÛŒÚ© Ú¯Ø±Ù‡ Ø§Ø² Ú¯Ø±Ø§Ù"""
        if node_id not in self.graph:
            return {}
        
        context = {
            'neighbors': [],
            'paths': [],
            'subgraph': {}
        }
        
        # Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…
        for neighbor in self.graph.neighbors(node_id):
            edge_data = self.graph.get_edge_data(node_id, neighbor)
            if edge_data:
                context['neighbors'].append({
                    'id': neighbor,
                    'type': self.nodes[neighbor].type.value if neighbor in self.nodes else 'unknown',
                    'edge_type': list(edge_data.values())[0].get('type', 'unknown') if edge_data else 'unknown'
                })
        
        # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡
        if depth > 1:
            for other_node in list(self.graph.nodes())[:10]:  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª
                if other_node != node_id:
                    try:
                        path = nx.shortest_path(self.graph, node_id, other_node)
                        if len(path) <= depth + 1:
                            context['paths'].append({
                                'target': other_node,
                                'path': path,
                                'length': len(path) - 1
                            })
                    except (nx.NetworkXNoPath, nx.NodeNotFound):
                        pass
        
        return context
    
    async def find_related_concepts(self, concept: str, k: int = 5) -> List[Dict]:
        """ÛŒØ§ÙØªÙ† Ù…ÙØ§Ù‡ÛŒÙ… Ù…Ø±ØªØ¨Ø·"""
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ÙÙ‡ÙˆÙ…
        concept_results = await self.search(concept, k=1, node_type=NodeType.CONCEPT)
        
        if not concept_results:
            return []
        
        concept_id = concept_results[0]['node']['id']
        
        # ÛŒØ§ÙØªÙ† Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¯Ø± Ú¯Ø±Ø§Ù
        related = []
        
        if concept_id in self.graph:
            for neighbor in self.graph.neighbors(concept_id):
                if neighbor in self.nodes:
                    node = self.nodes[neighbor]
                    edge_data = self.graph.get_edge_data(concept_id, neighbor)
                    
                    related.append({
                        'id': neighbor,
                        'content': node.content,
                        'type': node.type.value,
                        'relationship': list(edge_data.values())[0].get('type', 'unknown') if edge_data else 'unknown',
                        'strength': node.importance
                    })
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù‡Ù…ÛŒØª
        related.sort(key=lambda x: x['strength'], reverse=True)
        
        return related[:k]
    
    async def get_knowledge_summary(self, topic: str) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø¯Ø§Ù†Ø´ Ø¯Ø± Ù…ÙˆØ±Ø¯ ÛŒÚ© Ù…ÙˆØ¶ÙˆØ¹"""
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ÙˆØ¶ÙˆØ¹
        results = await self.search(topic, k=20)
        
        summary = {
            'topic': topic,
            'main_concepts': [],
            'key_entities': [],
            'relationships': [],
            'documents': [],
            'confidence': 0.0
        }
        
        concept_count = 0
        entity_count = 0
        doc_count = 0
        
        for result in results:
            node = result['node']
            
            if node['type'] == NodeType.CONCEPT.value and concept_count < 5:
                summary['main_concepts'].append({
                    'concept': node['content'],
                    'relevance': result['similarity']
                })
                concept_count += 1
            
            elif node['type'] == NodeType.ENTITY.value and entity_count < 10:
                summary['key_entities'].append({
                    'entity': node['content']['text'] if isinstance(node['content'], dict) else node['content'],
                    'label': node['metadata'].get('label', 'unknown'),
                    'relevance': result['similarity']
                })
                entity_count += 1
            
            elif node['type'] == NodeType.DOCUMENT.value and doc_count < 5:
                summary['documents'].append({
                    'id': node['id'],
                    'summary': node['content'],
                    'relevance': result['similarity']
                })
                doc_count += 1
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø·
            if 'context' in result and result['context'].get('neighbors'):
                for neighbor in result['context']['neighbors']:
                    summary['relationships'].append({
                        'source': node['id'],
                        'target': neighbor['id'],
                        'type': neighbor['edge_type']
                    })
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
        if results:
            summary['confidence'] = sum(r['similarity'] for r in results) / len(results)
        
        return summary
    
    def _save_to_database(self, node: KnowledgeNode):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO nodes 
            (id, type, content, embedding, metadata, timestamp, access_count, importance, vector_id)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            node.id,
            node.type.value,
            pickle.dumps(node.content),
            node.embedding.tobytes() if node.embedding is not None else None,
            json.dumps(node.metadata),
            node.timestamp,
            node.access_count,
            node.importance,
            node.vector_id
        ))
        
        conn.commit()
        conn.close()
    
    def _save_edge_to_database(self, edge: KnowledgeEdge):
        """Ø°Ø®ÛŒØ±Ù‡ ÛŒØ§Ù„ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO edges (source, target, type, weight, metadata, timestamp)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            edge.source,
            edge.target,
            edge.type.value,
            edge.weight,
            json.dumps(edge.metadata),
            edge.timestamp
        ))
        
        conn.commit()
        conn.close()
    
    def _process_queue(self):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§Øª Ø³Ù†Ú¯ÛŒÙ†"""
        while self.running:
            try:
                item = self.processing_queue.get(timeout=1)
                if item['type'] == 'update_importance':
                    self._update_node_importance(item['node_id'])
                elif item['type'] == 'prune_graph':
                    self._prune_graph()
                elif item['type'] == 'reindex':
                    self._reindex()
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡: {e}")
    
    def _update_node_importance(self, node_id: str):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø§Ù‡Ù…ÛŒØª Ú¯Ø±Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªÙØ§Ø¯Ù‡"""
        if node_id in self.nodes:
            node = self.nodes[node_id]
            
            # Ø§ÙØ²Ø§ÛŒØ´ Ø§Ù‡Ù…ÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø³ØªØ±Ø³ÛŒ
            node.access_count += 1
            node.importance = min(1.0, node.importance + 0.01)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            self._save_to_database(node)
    
    def _prune_graph(self, threshold: float = 0.1):
        """Ù‡Ø±Ø³ Ú¯Ø±Ø§Ù Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù…â€ŒØ§Ù‡Ù…ÛŒØª"""
        with self.write_lock:
            nodes_to_remove = []
            
            for node_id, node in self.nodes.items():
                if node.importance < threshold and node.access_count < 5:
                    nodes_to_remove.append(node_id)
            
            for node_id in nodes_to_remove:
                if node_id in self.graph:
                    self.graph.remove_node(node_id)
                if node_id in self.nodes:
                    del self.nodes[node_id]
                
                # Ø­Ø°Ù Ø§Ø² Ø§ÛŒÙ†Ø¯Ú©Ø³
                if node_id in self.index_to_id.values():
                    # TODO: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø°Ù Ø§Ø² FAISS
                    pass
            
            self.stats['total_nodes'] -= len(nodes_to_remove)
            logger.info(f"ğŸ§¹ {len(nodes_to_remove)} Ú¯Ø±Ù‡ Ú©Ù…â€ŒØ§Ù‡Ù…ÛŒØª Ù‡Ø±Ø³ Ø´Ø¯Ù†Ø¯")
    
    def _reindex(self):
        """Ø¨Ø§Ø²Ø§ÛŒÙ†Ø¯Ú©Ø³ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ú¯Ø±Ù‡â€ŒÙ‡Ø§"""
        with self.index_lock:
            # Ø¨Ø§Ø²Ù†Ø´Ø§Ù†ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§
            self.index = faiss.IndexFlatIP(self.dimension)
            self.hnsw_index = hnswlib.Index(space='ip', dim=self.dimension)
            self.hnsw_index.init_index(max_elements=len(self.nodes) + 1000, ef_construction=200, M=48)
            
            self.index_to_id = {}
            
            # Ø§ÙØ²ÙˆØ¯Ù† Ù…Ø¬Ø¯Ø¯ Ù‡Ù…Ù‡ Ú¯Ø±Ù‡â€ŒÙ‡Ø§
            for i, (node_id, node) in enumerate(self.nodes.items()):
                if node.embedding is not None:
                    self.index.add(np.array([node.embedding]))
                    self.hnsw_index.add_items(np.array([node.embedding]), np.array([i]))
                    node.vector_id = i
                    self.index_to_id[i] = node_id
            
            logger.info(f"ğŸ”„ Ø¨Ø§Ø²Ø§ÛŒÙ†Ø¯Ú©Ø³ Ú©Ø§Ù…Ù„ Ø´Ø¯: {len(self.nodes)} Ú¯Ø±Ù‡")
    
    def get_statistics(self) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ú©Ø§Ù…Ù„"""
        return {
            'total_nodes': self.stats['total_nodes'],
            'total_edges': self.stats['total_edges'],
            'graph_stats': {
                'nodes': self.graph.number_of_nodes(),
                'edges': self.graph.number_of_edges(),
                'density': nx.density(self.graph)
            },
            'index_stats': {
                'faiss_size': self.index.ntotal,
                'hnsw_size': self.hnsw_index.get_current_count()
            },
            'performance': {
                'queries': self.stats['queries_performed'],
                'avg_query_time': self.stats['avg_query_time'],
                'cache_hit_rate': self.stats['cache_hits'] / (self.stats['cache_hits'] + self.stats['cache_misses']) if (self.stats['cache_hits'] + self.stats['cache_misses']) > 0 else 0
            },
            'cache_size': len(self.cache.keys())
        }
    
    def export_graph(self, format: str = 'json') -> str:
        """Ø®Ø±ÙˆØ¬ÛŒ Ú¯Ø±ÙØªÙ† Ø§Ø² Ú¯Ø±Ø§Ù"""
        if format == 'json':
            data = {
                'nodes': [
                    {
                        'id': n.id,
                        'type': n.type.value,
                        'content': str(n.content)[:100],
                        'importance': n.importance
                    }
                    for n in self.nodes.values()
                ],
                'edges': [
                    {
                        'source': e.source,
                        'target': e.target,
                        'type': e.type.value,
                        'weight': e.weight
                    }
                    for e in self.edges
                ]
            }
            return json.dumps(data, indent=2)
        
        elif format == 'graphml':
            return ''.join(nx.generate_graphml(self.graph))
        
        return ""

class DocumentProcessor:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ù†Ø´"""
    
    def __init__(self, knowledge_graph: AdvancedKnowledgeGraph):
        self.knowledge_graph = knowledge_graph
        self.supported_formats = ['txt', 'pdf', 'docx', 'md', 'csv', 'json']
        self.processing_queue = asyncio.Queue()
        
    async def process_document_batch(self, file_paths: List[str]) -> List[str]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø§Ø³Ù†Ø§Ø¯"""
        tasks = []
        for file_path in file_paths:
            task = asyncio.create_task(self.process_single_document(file_path))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return results
    
    async def process_single_document(self, file_path: str) -> str:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ø³Ù†Ø¯"""
        logger.info(f"ğŸ“„ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´: {file_path}")
        
        try:
            # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„
            content = await self._read_file(file_path)
            
            # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
            cleaned_content = self._preprocess_text(content)
            
            # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ù‡ Ú¯Ø±Ø§Ù Ø¯Ø§Ù†Ø´
            doc_id = await self.knowledge_graph.add_document(
                cleaned_content,
                metadata={'source': file_path, 'type': Path(file_path).suffix}
            )
            
            logger.info(f"âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯: {file_path} -> {doc_id}")
            return doc_id
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {file_path}: {e}")
            return ""
    
    async def _read_file(self, file_path: str) -> str:
        """Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø¨Ø§ ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        ext = Path(file_path).suffix.lower()
        
        if ext == '.txt' or ext == '.md':
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        
        elif ext == '.pdf':
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ PDF
            import PyPDF2
            text = ""
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    text += page.extract_text()
            return text
        
        elif ext == '.docx':
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ DOCX
            import docx
            doc = docx.Document(file_path)
            return '\n'.join([paragraph.text for paragraph in doc.paragraphs])
        
        elif ext == '.csv':
            import pandas as pd
            df = pd.read_csv(file_path)
            return df.to_string()
        
        elif ext == '.json':
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return json.dumps(data, indent=2)
        
        return ""
    
    def _preprocess_text(self, text: str) -> str:
        """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†"""
        # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        import re
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\.\,\?\!]', '', text)
        
        # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø·ÙˆÙ„
        if len(text) > 10000:
            text = text[:10000]
        
        return text.strip()

# Ù†Ù…ÙˆÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØªØ³Øª
if __name__ == "__main__":
    import asyncio
    
    async def test():
        kg = AdvancedKnowledgeGraph()
        processor = DocumentProcessor(kg)
        
        # Ø§ÙØ²ÙˆØ¯Ù† ÛŒÚ© Ø³Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡
        doc_id = await kg.add_document(
            "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø´Ø§Ø®Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¹Ù„ÙˆÙ… Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ Ø³Ø§Ø®Øª Ù…Ø§Ø´ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²Ø¯.",
            metadata={'source': 'test', 'type': 'txt'}
        )
        
        print(f"Ø³Ù†Ø¯ Ø§ÙØ²ÙˆØ¯Ù‡ Ø´Ø¯: {doc_id}")
        
        # Ø¬Ø³ØªØ¬Ùˆ
        results = await kg.search("Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ", k=5)
        print(f"Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ: {len(results)}")
        
        # Ø¢Ù…Ø§Ø±
        stats = kg.get_statistics()
        print(f"Ø¢Ù…Ø§Ø±: {stats}")
    
    asyncio.run(test())
